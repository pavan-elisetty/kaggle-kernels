{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"nfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")\n\nnp.random.seed(0)\n#setting seed for reproducibility","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = nfl_data.isnull().sum()\nmissing_values_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data[1:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_cells = np.product(nfl_data.shape)\ntotal_missing = missing_values_count.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_missing=missing_values_count.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percent of data that is missing\npercent_missing = (total_missing/total_cells) * 100\nprint(percent_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nfl_data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all columns with at least one missing value\ncolumns_with_na_dropped = nfl_data.dropna(axis=1)\ncolumns_with_na_dropped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# just how much data did we lose?\nprint(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])\nprint(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a small subset of the NFL dataset\nsubset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()\nsubset_nfl_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all NA's with 0\nsubset_nfl_data.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace all NA's the value that comes directly after it in the same column, \n# then replace all the remaining na's with 0\nsubset_nfl_data.fillna(method='bfill', axis=0).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# generating some numbers use np.arange","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats #BOX_COX transformation\nfrom mlxtend.preprocessing import minmax_scaling\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=list(np.arange(1,100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b = np.random.exponential(size=1000)\nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#generating 1000 data points randomly\n\ndata = np.random.exponential(size=1000)\n\n#min_max between 0 and 1 \nscaled_data = minmax_scaling(data , columns=[0])\n\n#plotting both together to compare\n\nfig , ax = plt.subplots(1,2)\n\nsns.distplot(data,ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal = stats.boxcox(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normal[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\nimport pandas as pd\n\n\nlandslides_data = pd.read_csv('../input/landslide-events/catalog.csv')\nlandslides_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides_data['date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"landslides_data['Dates']= pd.to_datetime(landslides_data['date'] , format = '%m/%d/%y')\nlandslides_data=landslides_data.drop('date',axis=1)\nlandslides_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months=landslides_data['Dates'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"months = months.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(months , kde=False , bins=31 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport chardet #character encoding module\nnp.random.seed(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"before = \"This is the euro symbol: €\"\ntype(before)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"after = before.encode('utf-8' , errors='replace')\ntype(after)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"after","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(after.decode('utf-8'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(after.decode('ascii'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"before = \"This is the euro symbol: €\"\nafter = before.encode(\"ascii\", errors = \"replace\")\n\nprint(after.decode(\"ascii\"))\n#lost our original form\n#This is bad and we want to avoid doing it! It's far better to convert all our text to UTF-8 as soon as we can and keep it in that encoding.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# try to read in a file not in UTF-8\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/kickstarter-projects/ks-projects-201612.csv','rb') as rawdata:\n    result = chardet.detect(rawdata.read(20000))\nprint(result)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read in the file with the encoding detected by chardet\nkickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')\n\n# look at the first few lines\nkickstarter_2016.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save our file (will be saved as UTF-8 by default!)\nkickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport chardet\nimport fuzzywuzzy\nfrom fuzzywuzzy import process\nnp.random.seed(0)\nprofessors = pd.read_csv('../input/pakistan-intellectual-capital/pakistan_intellectual_capital.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"professors.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   Unnamed: 0  S#         Teacher Name  \\\n0           2   3      Dr. Abdul Basit   \n1           4   5      Dr. Waheed Noor   \n2           5   6     Dr. Junaid Baber   \n3           6   7  Dr. Maheen Bakhtyar   \n4          24  25          Samina Azim   \n\n            University Currently Teaching             Department  \\\n0               University of Balochistan  Computer Science & IT   \n1               University of Balochistan  Computer Science & IT   \n2               University of Balochistan  Computer Science & IT   \n3               University of Balochistan  Computer Science & IT   \n4  Sardar Bahadur Khan Women's University       Computer Science   \n\n  Province University Located          Designation Terminal Degree  \\\n0                 Balochistan  Assistant Professor             PhD   \n1                 Balochistan  Assistant Professor             PhD   \n2                 Balochistan  Assistant Professor             PhD   \n3                 Balochistan  Assistant Professor             PhD   \n4                 Balochistan             Lecturer              BS   \n\n                                      Graduated from   Country    Year  \\\n0                      Asian Institute of Technology  Thailand     NaN   \n1                      Asian Institute of Technology  Thailand     NaN   \n2                      Asian Institute of Technology  Thailand     NaN   \n3                      Asian Institute of Technology  Thailand     NaN   \n4  Balochistan University of Information Technolo...  Pakistan  2005.0   \n\n           Area of Specialization/Research Interests Other Information  \n0                        Software Engineering & DBMS               NaN  \n1                                               DBMS               NaN  \n2          Information processing, Multimedia mining               NaN  \n3  NLP, Information Retrieval, Question Answering...               NaN  \n4                      VLSI Electronics DLD Database               NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>S#</th>\n      <th>Teacher Name</th>\n      <th>University Currently Teaching</th>\n      <th>Department</th>\n      <th>Province University Located</th>\n      <th>Designation</th>\n      <th>Terminal Degree</th>\n      <th>Graduated from</th>\n      <th>Country</th>\n      <th>Year</th>\n      <th>Area of Specialization/Research Interests</th>\n      <th>Other Information</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>3</td>\n      <td>Dr. Abdul Basit</td>\n      <td>University of Balochistan</td>\n      <td>Computer Science &amp; IT</td>\n      <td>Balochistan</td>\n      <td>Assistant Professor</td>\n      <td>PhD</td>\n      <td>Asian Institute of Technology</td>\n      <td>Thailand</td>\n      <td>NaN</td>\n      <td>Software Engineering &amp; DBMS</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>5</td>\n      <td>Dr. Waheed Noor</td>\n      <td>University of Balochistan</td>\n      <td>Computer Science &amp; IT</td>\n      <td>Balochistan</td>\n      <td>Assistant Professor</td>\n      <td>PhD</td>\n      <td>Asian Institute of Technology</td>\n      <td>Thailand</td>\n      <td>NaN</td>\n      <td>DBMS</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>6</td>\n      <td>Dr. Junaid Baber</td>\n      <td>University of Balochistan</td>\n      <td>Computer Science &amp; IT</td>\n      <td>Balochistan</td>\n      <td>Assistant Professor</td>\n      <td>PhD</td>\n      <td>Asian Institute of Technology</td>\n      <td>Thailand</td>\n      <td>NaN</td>\n      <td>Information processing, Multimedia mining</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>7</td>\n      <td>Dr. Maheen Bakhtyar</td>\n      <td>University of Balochistan</td>\n      <td>Computer Science &amp; IT</td>\n      <td>Balochistan</td>\n      <td>Assistant Professor</td>\n      <td>PhD</td>\n      <td>Asian Institute of Technology</td>\n      <td>Thailand</td>\n      <td>NaN</td>\n      <td>NLP, Information Retrieval, Question Answering...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>25</td>\n      <td>Samina Azim</td>\n      <td>Sardar Bahadur Khan Women's University</td>\n      <td>Computer Science</td>\n      <td>Balochistan</td>\n      <td>Lecturer</td>\n      <td>BS</td>\n      <td>Balochistan University of Information Technolo...</td>\n      <td>Pakistan</td>\n      <td>2005.0</td>\n      <td>VLSI Electronics DLD Database</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries=professors['Country'].unique()\ncountries.sort()\ncountries","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"array([' Germany', ' New Zealand', ' Sweden', ' USA', 'Australia',\n       'Austria', 'Canada', 'China', 'Finland', 'France', 'Greece',\n       'HongKong', 'Ireland', 'Italy', 'Japan', 'Macau', 'Malaysia',\n       'Mauritius', 'Netherland', 'New Zealand', 'Norway', 'Pakistan',\n       'Portugal', 'Russian Federation', 'Saudi Arabia', 'Scotland',\n       'Singapore', 'South Korea', 'SouthKorea', 'Spain', 'Sweden',\n       'Thailand', 'Turkey', 'UK', 'USA', 'USofA', 'Urbana', 'germany'],\n      dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"professors['Country']=professors['Country'].str.lower()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing tailing whitespaces\nprofessors['Country']=professors['Country'].str.strip()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countries = professors['Country'].unique()\ncountries.sort()\ncountries\n#southkorea and south korea inconsistency","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"array(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n       'norway', 'pakistan', 'portugal', 'russian federation',\n       'saudi arabia', 'scotland', 'singapore', 'south korea',\n       'southkorea', 'spain', 'sweden', 'thailand', 'turkey', 'uk',\n       'urbana', 'usa', 'usofa'], dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"professors['Country'].nunique()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"34"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the top 10 closest matches to \"south korea\"\nmatches = fuzzywuzzy.process.extract('south korea' , countries , limit=10 , scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\nmatches","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"[('south korea', 100),\n ('southkorea', 48),\n ('saudi arabia', 43),\n ('norway', 35),\n ('austria', 33),\n ('ireland', 33),\n ('pakistan', 32),\n ('portugal', 32),\n ('scotland', 32),\n ('australia', 30)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio = 47):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio > 90\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the function we just wrote to replace close matches to \"south korea\" with \"south korea\"\nreplace_matches_in_column(df=professors, column='Country', string_to_match=\"south korea\")","execution_count":20,"outputs":[{"output_type":"stream","text":"All done!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get all the unique values in the 'Country' column\ncountries = professors['Country'].unique()\n\n# sort them alphabetically and then take a closer look\ncountries.sort()\ncountries\n","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"array(['australia', 'austria', 'canada', 'china', 'finland', 'france',\n       'germany', 'greece', 'hongkong', 'ireland', 'italy', 'japan',\n       'macau', 'malaysia', 'mauritius', 'netherland', 'new zealand',\n       'norway', 'pakistan', 'portugal', 'russian federation',\n       'saudi arabia', 'scotland', 'singapore', 'south korea', 'spain',\n       'sweden', 'thailand', 'turkey', 'uk', 'urbana', 'usa', 'usofa'],\n      dtype=object)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"professors['Country'].nunique()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}