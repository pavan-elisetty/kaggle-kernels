{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"#NATURAL LANGUAGE PROCESSING\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en')\ndoc = nlp(\"Tea is healthy and calming, don't you think?\")\nfor token in doc:\n    print(token)\nprint(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\nprint(\"-\"*40)\nfor token in doc:\n    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import PhraseMatcher\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n\"\"\"To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the PhraseMatcher itself.\nThe matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitive matching.\n\nNext you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model.\n\"\"\"\nterms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\npatterns = [nlp(text) for text in terms]\nmatcher.add(\"TerminologyList\", None, *patterns)\n#Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text.\n\n# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\ntext_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n               \"photography tests pitting the iPhone 11 Pro against the \"\n               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\") \nmatches = matcher(text_doc)\nprint(matches)\n#The matches here are a tuple of the match id and the positions of the start and end of the phrase.\nmatch_id, start, end = matches[0]\nprint(nlp.vocab.strings[match_id], text_doc[start:end])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
